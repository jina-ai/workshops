{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08NvzS3_nCXl"
   },
   "source": [
    "# Finetuner X CLIP Benchmark\n",
    "\n",
    "@bo_wangbo\n",
    "@fissoreg\n",
    "\n",
    "In this Colab notebook, we'll try to use [Finetuner](https://github.com/jina-ai/finetuner) to fine-tune the CLIP model on `Flickr8k`, and compare the retrieval metrics produced by the fine-tuned model against pre-trained zero-shot results produced from CLIP Benchmark.\n",
    "\n",
    "*NOTE: Finetuner is a cloud-based training platform, which requires you to login and Finetuner will allocate computational resources automatically for free.*\n",
    "\n",
    "**Please Consider [Switching to a GPU Runtime](https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403) for faster evaluation!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFB3KT4a59mn"
   },
   "outputs": [],
   "source": [
    "!pip install \"finetuner[full]\"\n",
    "# our fork of CLIP benchmark, resolved some minor issues in data builder and adjust the evaluator code to allow evaluator receive 2 models\n",
    "# when fine-tuning CLIP, Finetuner will un-wrap the CLIP model into 2 models and save them individually\n",
    "!pip install kaggle\n",
    "!pip install git+https://github.com/bwanglzu/CLIP_benchmark.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol2JJrSJsD-q"
   },
   "source": [
    "## Preparing the training data\n",
    "\n",
    "CLIP Benchmark comes with a dataset `builder` that does much of the work of assembling training data. However, for Finetuner, we need to convert it into Jina DocArray format. \n",
    "\n",
    "We will use:\n",
    "\n",
    "1. CLIP Benchmark contains a file named `captions.txt` which includes all Flickr8k image urls with captions.\n",
    "2. CLIP Benchmark reused the Karpathy split which split the `Flickr8k` into test sets and training sets. The test set includs 5000 images with annotations.\n",
    "\n",
    "We will build our training set by loading all images, and then then excluding the test set images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUbvP-jGD74b",
    "outputId": "2db3f3b5-fe80-4384-b694-5f96bc3d08c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Flickr\n",
       "    Number of datapoints: 1000\n",
       "    Root location: root"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from clip_benchmark.datasets.builder import build_dataset\n",
    "\n",
    "# please fill in your kaggle token here, you should be able to get your kaggle \n",
    "# user name and key in kaggle personal settings.\n",
    "# CLIP Benchmark uses kaggle to download flickr8k dataset\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "build_dataset(dataset_name='flickr8k', annotation_file=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8RAM-4UD9aF",
    "outputId": "15558fce-9166-4827-9985-1134f617954c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the full image set is 40455\n",
      "Size of the test image set is 5000\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/content/root/'\n",
    "full_annotation = root_dir + 'captions.txt'\n",
    "test_annotation = root_dir + 'flickr8k_test_karpathy.txt'\n",
    "\n",
    "all_imgs = []\n",
    "test_imgs = []\n",
    "with open(full_annotation, 'r') as f:\n",
    "    next(f) # exclude the header line\n",
    "    for idx, item in enumerate(f.readlines()):\n",
    "        all_imgs.append(item.split(',', 1)[0])\n",
    "\n",
    "with open(test_annotation, 'r') as f:\n",
    "    next(f) # exclude the header line\n",
    "    for idx, item in enumerate(f.readlines()):\n",
    "        test_imgs.append(item.split(',', 1)[0])\n",
    "\n",
    "print(f'Size of the full image set is {len(all_imgs)}')\n",
    "print(f'Size of the test image set is {len(test_imgs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG3M2Cv7wfKU"
   },
   "source": [
    "Now we will convert the downloaded images into `DocumentArray` format like this:\n",
    "\n",
    "```python\n",
    "from docarray import Document, DocumentArray\n",
    "\n",
    "pairs = DocumentArray()\n",
    "pair_1 = Document(chunks=[\n",
    "    img_chunk = Document(uri='your-image.jpg', modality='image'),\n",
    "    txt_chunk = Document(content='the text descriptor', modality='text'),\n",
    "]}\n",
    "pair_2 = ...\n",
    "pairs.extend([pair_1, pair_2, ...])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLgNsciPvLlH",
    "outputId": "77538bb0-d0d8-4959-f723-43ace87630d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:30, 166.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training data is 4376\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from docarray import Document, DocumentArray\n",
    "\n",
    "train = DocumentArray()\n",
    "with open(full_annotation, 'r') as f:\n",
    "    next(f) # exclude the header line\n",
    "    for idx, line in tqdm(enumerate(f.readlines())):\n",
    "        url, txt = line.split(',', 1)\n",
    "        if url in test_imgs:  # do not include test images into training set\n",
    "            continue\n",
    "        img_chunk = Document(uri=root_dir + url, modality='image')\n",
    "        txt_chunk = Document(content=txt, modality='text')\n",
    "        img_chunk.load_uri_to_image_tensor(224, 224)\n",
    "        img_chunk.pop('uri')\n",
    "        pair = Document(chunks=[img_chunk, txt_chunk])\n",
    "        train.append(pair)\n",
    "        if idx == 5000: # we only use a subset to train\n",
    "            break\n",
    "\n",
    "print(f'The size of the training data is {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhXVm8tV0VEr"
   },
   "source": [
    "The Flickr8k dataset contains 8,000 images, each with 5 descriptive texts, or 40,000 image-text pairs in total.\n",
    "\n",
    "+ The training set has ~35000 image-text pairs.\n",
    "+ The test set has ~5000 image-text pairs.\n",
    "\n",
    "## Start Fine-tuning\n",
    "\n",
    "Now that we have prepared the training and test data, the next step is to start the fine-tuning job using Finetuner. Finetuner takes a pre-trained model from a 3rd party library, such as `open_clip`, then jointly optimize the `CLIPLoss` function for the image encoder and text encoder.\n",
    "\n",
    "Finetuner will also reserve a cloud GPU for you for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203,
     "referenced_widgets": [
      "5640c168070347138c6f12a89f3527f3",
      "74d5ed1405e14e7b82fa5c439509b08d",
      "d22020b9cdcb4db5b903ec0510d77bf9",
      "7a2cb071664a4ec985769c655941b3ab",
      "c7ba7e7ad7a1478498bd1b7035cfa262",
      "052b3cbaaf854241a5a9006b44b4164d",
      "c8429c73d5554f7e9f6578a7c04557af",
      "85713ec3389446e6869e69a262023f43",
      "cdd962679dbb49e3873289ef6bd13633",
      "f4b7f904482d4e5d9ab526fe7897f256",
      "f27061fd69d24cd6a77a694aec8906f2",
      "b2492a07ae4649f29c0d0a8c23c4047e",
      "0e7d0228b5b24d0ba23a493264adb815",
      "f75a1fb831f64ca59dfeb7ed8e411483",
      "a56509f7c23d495fa81dc07b5ed38419",
      "14fdf26fefb4453381b45dd79a53b42f",
      "668ad79a393f4d21bf601b2354ae3e84",
      "7ea64cf5a7064a5fbf870f46616187e6",
      "59221f747d3d472a8eb4c7e8a819bf71",
      "5361787acc0948be974a31bd054632b3",
      "35202cf8b3b64938b0437e3433ee7d1e",
      "2e16156e982840e7bcf859bc8bac43c0"
     ]
    },
    "id": "hG_siRL7zvEp",
    "outputId": "0b5bde65-8598-410f-a0da-90fcffdcc962"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59221f747d3d472a8eb4c7e8a819bf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n<div class='custom-container'>\\n    <style>\\n        .custom-container {\\n       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import finetuner\n",
    "\n",
    "finetuner.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrFZqjvY26eJ"
   },
   "outputs": [],
   "source": [
    "# Note, we have push the training set below to the cloud, and set the dataset as public, so you don't have to push again.\n",
    "# train.push('finetuner-flickr8k-demo', public=True, show_progress=True)\n",
    "# finetuner.delete_run('clip-run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7BbFKF71c-l"
   },
   "outputs": [],
   "source": [
    "run = finetuner.fit(\n",
    "    model='ViT-B-32::openai', # we take ViT-B-32 trained from Open AI, model provided by OpenCLIP\n",
    "    train_data='finetuner-flickr8k-demo', # the dataset we prepared has been pushed to the cloud in the prev section\n",
    "    run_name='clip-run',\n",
    "    loss='CLIPLoss', # use CLIPLoss for fine-tuning CLIP model\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    learning_rate= 1e-6,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "77fd7e85ea4346c5bb3886f2f4c42bca",
      "eedc32bcc61a430dadec9bca5a508745"
     ]
    },
    "id": "kGVQAWnh25SK",
    "outputId": "7aadb5ca-bad4-4081-ceea-74ed401a44c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fd7e85ea4346c5bb3886f2f4c42bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# takes around ~10 minutes to finish\n",
    "for log_entry in run.stream_logs():\n",
    "    print(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV5tKv0sUoBa"
   },
   "source": [
    "## Inference\n",
    "\n",
    "After fine-tuning is finished, your fine-tuned model is saved in the cloud as an `artifact`. An `artifact` contains the model weights, and some metadata such as evaluation metrics and hyper-parameters.\n",
    "\n",
    "In order to download your artifact, call the method `run.save_artifact()`.\n",
    "\n",
    "Since CLIP is actually two models and we are fine-tuning them in parallel, there will be two models downloaded as one artifact: a text encoder and an image encoder. To use these models to do encodings, you will need the `finetuner.get_model()` with a `select_model` -- either `clip-text` or `clip-vision` -- get access to CLIPs constituent models individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "da795a9e9c8c47c489b520483c95713a",
      "f33b28cfdc71491baa49f843b1f51389"
     ]
    },
    "id": "Do1nK3sY2OS7",
    "outputId": "50a6b94b-335b-4427-dcfd-9a74e8787b60"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da795a9e9c8c47c489b520483c95713a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 354M/354M [00:01<00:00, 267MiB/s]\n"
     ]
    }
   ],
   "source": [
    "artifact = run.save_artifact('clip-model')\n",
    "\n",
    "clip_txt_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
    "clip_img_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnlCnAiPZLjm"
   },
   "source": [
    "With these two models and Finetuner, you can encode your image and text data with:\n",
    "\n",
    "```python\n",
    "data = DocumentArray([Document(content='some text to encode')])\n",
    "finetuner.encode(model=clip_txt_encoder, data=data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRDeIQRe2aiG"
   },
   "source": [
    "In order to use CLIP Benchmark, we must provide a PyTorch file rather than a Finetuner inference runtime. The code below is a hack to overcome this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q503pCw4bS4E",
    "outputId": "a2c85428-bfe5-4c87-8e2a-46918a509f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!unzip clip-model/clip-run.zip # as said, artifact are saved as zip together with weights and some metadata.\n",
    "import torch\n",
    "from _finetuner.models.builders import OpenCLIPVisionBuilder, OpenCLIPTextBuilder\n",
    "\n",
    "clip_vision = OpenCLIPVisionBuilder(descriptor='ViT-B-32::openai').build()\n",
    "clip_vision.load_state_dict(torch.load(f'/content/{run.name}/models/clip-vision/model.pt'))\n",
    "\n",
    "clip_text = OpenCLIPTextBuilder(descriptor='ViT-B-32::openai').build()\n",
    "clip_text.load_state_dict(torch.load(f'/content/{run.name}/models/clip-text/model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j_H2gzY8zGW"
   },
   "source": [
    "Then we can run CLIP benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnbGx73pbxFU",
    "outputId": "d1f01226-28af-45e5-c7a3-aeeeecc03460"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "16it [00:16,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'flickr8k',\n",
      " 'metrics': {'image_retrieval_recall@5': 0.8537999987602234,\n",
      "             'text_retrieval_recall@5': 0.9100000262260437},\n",
      " 'model': 'ViT-B-32',\n",
      " 'pretrained': 'openai',\n",
      " 'task': 'finetuned'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Console script for clip_benchmark. \n",
    "Code copied from CLIP Benchmark with minor adjusts to run in colab.\n",
    "\"\"\"\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import open_clip\n",
    "from pprint import pprint\n",
    "\n",
    "from clip_benchmark.datasets.builder import build_dataset, get_dataset_collate_fn\n",
    "from clip_benchmark.metrics import  zeroshot_retrieval\n",
    "\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_encoder = clip_vision.to(device)\n",
    "text_encoder = clip_text.to(device)\n",
    "_, _, transform = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "dataset = build_dataset(\n",
    "    dataset_name='flickr8k',\n",
    "    root='root',\n",
    "    transform=transform,\n",
    "    split='test',\n",
    "    annotation_file=None,\n",
    "    download=True,\n",
    ")\n",
    "collate_fn = get_dataset_collate_fn('flickr8k')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=64,\n",
    "    shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "metrics = zeroshot_retrieval.evaluate(\n",
    "    image_encoder,\n",
    "    text_encoder,\n",
    "    dataloader,\n",
    "    open_clip.tokenizer.tokenize,\n",
    "    recall_k_list=[5],\n",
    "    device=device,\n",
    "    amp=True\n",
    ")\n",
    "dump = {\n",
    "    \"dataset\": 'flickr8k',\n",
    "    \"model\": 'ViT-B-32',\n",
    "    \"pretrained\": 'openai',\n",
    "    \"task\": 'finetuned',\n",
    "    \"metrics\": metrics\n",
    "}\n",
    "pprint(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODbzFgVGlBd_"
   },
   "source": [
    "## Results: Pre-Trained Zero-Shot vs Fine-Tuned\n",
    "\n",
    "The people responsible for CLIP Benchmark have published benchmarking results for a wide variety of models and configuarations in [this csv](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/benchmark.csv).\n",
    "\n",
    "For simplicity, we show the comparsion below:\n",
    "\n",
    "+ `image_retrieval_recall@5`: use text queries to find top 5 similar images.\n",
    "+ `text_retrieval_recall@5`: use image to find top 5 similar text.\n",
    "\n",
    "\n",
    "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
    "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
    "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8537999987602234| 0.9100000262260437 |\n",
    "\n",
    "Apart from that, we have done some extensive experiments on three datasets, these are the results we get:\n",
    "\n",
    "\n",
    "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
    "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
    "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8651999831199646| 0.9079999923706055 |\n",
    "| ViT-B-16-plus-240                | flickr8k      |0.6441478133201599 | 0.7916203141212463   |0.8784000277519226| 0.9200000166893005 |\n",
    "| ViT-B-32-quickgelu#laion400m_e32 | flickr8k      |0.5787171125411987 | 0.7392163872718811   |0.849399983882904 | 0.9020000100135803 |\n",
    "| ViT-B-32#openai                  | flickr30k     |0.8338000178337097 | 0.9490000009536743   |0.9016000032424927| 0.9480000138282776 |\n",
    "| ViT-B-16-plus-240                | flickr30k     |0.8894000053405762 | 0.9710000157356262   |0.9169999957084656| 0.9710000157356262 |\n",
    "| ViT-B-32-quickgelu#laion400m_e32 | flickr30k     |0.8546000123023987 | 0.9409999847412109   |0.8715999722480774| 0.9290000200271606 |\n",
    "| ViT-B-32#openai                  | coco captions |0.5584565997123718 | 0.748199999332428    |0.6546581387519836| 0.7454000115394592 |\n",
    "| ViT-B-16-plus-240                | coco captions |0.6620951890945435 | 0.8101999759674072   |0.7120751738548279| 0.8136000037193298 |\n",
    "| ViT-B-32-quickgelu#laion400m_e32 | coco captions |0.6084766387939453 | 0.7675999999046326   |0.6713714599609375| 0.7635999917984009 |\n",
    "\n",
    "Our Finetuner hyper-parameters were: `learning_rate: 1e-6`, `epochs: 5`, `optimizer: Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIPy1Bh5lxGf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052b3cbaaf854241a5a9006b44b4164d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e7d0228b5b24d0ba23a493264adb815": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": "10px 0 0 0",
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "300px"
     }
    },
    "14fdf26fefb4453381b45dd79a53b42f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_668ad79a393f4d21bf601b2354ae3e84",
      "placeholder": "​",
      "style": "IPY_MODEL_7ea64cf5a7064a5fbf870f46616187e6",
      "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt=\"Jina AI\">\n        <div class='spaced'></div>\n        <p>\n            Please open <a href='https://jina-ai.us.auth0.com/authorize?response_mode=form_post&nonce=fe52fd0fa296f2302b33aab321a97623&state=fe52fd0fa296f2302b33aab321a97623&scope=profile+openid+email&redirect_uri=https%3A%2F%2Fapi.hubble.jina.ai%2Fv2%2Foidc%2FidpAuthorized&client_id=7pXAUAtiRqruNd6KJ6U3Zd9uhk5oLqZA&response_type=code&code_challenge=_XDNsaIW8OxdXBPEROkXG-mMtZlynYZuII8TN_l-SUA&code_challenge_method=S256' target='_blank'>this link</a> to continue the login process.\n        </p>\n    </center>\n</div>\n"
     }
    },
    "2e16156e982840e7bcf859bc8bac43c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35202cf8b3b64938b0437e3433ee7d1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5361787acc0948be974a31bd054632b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35202cf8b3b64938b0437e3433ee7d1e",
      "placeholder": "​",
      "style": "IPY_MODEL_2e16156e982840e7bcf859bc8bac43c0",
      "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            You are logged in to Jina AI!\n        </p>\n        <p>\n            If you want to log in again, run <code>notebook_login(force=True)</code>.\n        </p>\n    </center>\n</div>\n"
     }
    },
    "5640c168070347138c6f12a89f3527f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74d5ed1405e14e7b82fa5c439509b08d",
       "IPY_MODEL_d22020b9cdcb4db5b903ec0510d77bf9",
       "IPY_MODEL_7a2cb071664a4ec985769c655941b3ab",
       "IPY_MODEL_c7ba7e7ad7a1478498bd1b7035cfa262"
      ],
      "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
     }
    },
    "59221f747d3d472a8eb4c7e8a819bf71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5361787acc0948be974a31bd054632b3"
      ],
      "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
     }
    },
    "668ad79a393f4d21bf601b2354ae3e84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74d5ed1405e14e7b82fa5c439509b08d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8429c73d5554f7e9f6578a7c04557af",
      "placeholder": "​",
      "style": "IPY_MODEL_85713ec3389446e6869e69a262023f43",
      "value": "\n<div class='custom-container'>\n    <style>\n        .button1 {\n            color: white;\n            background-color: #009191;\n            border: 1px solid #009191;\n        }\n        .button2 {\n            color: #009191;\n            background-color: white;\n            border: 1px solid #009191;\n        }\n        .link1 {\n            color:#009191;\n            position: relative;\n            top: 22px;\n            right: -120px;\n            z-index: 99;\n        }\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: -10px;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            Copy a <b>Personal Access Token</b>, paste it below, and press the <b>Token login</b> button.\n            <br>\n            If you don't have a token, press the <b>Browser login</b> button to log in via the browser.\n        </p>\n        <a\n            href='https://hub.jina.ai/user/tokens'\n            target='__blank'\n            class='link1'>\n                Create\n        </a>\n    </center>\n</div>\n"
     }
    },
    "77fd7e85ea4346c5bb3886f2f4c42bca": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_eedc32bcc61a430dadec9bca5a508745",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠦</span> Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n</pre>\n",
         "text/plain": "\u001b[32m⠦\u001b[0m Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "7a2cb071664a4ec985769c655941b3ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [
       "button1"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Token login",
      "disabled": true,
      "icon": "",
      "layout": "IPY_MODEL_f27061fd69d24cd6a77a694aec8906f2",
      "style": "IPY_MODEL_b2492a07ae4649f29c0d0a8c23c4047e",
      "tooltip": ""
     }
    },
    "7ea64cf5a7064a5fbf870f46616187e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85713ec3389446e6869e69a262023f43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a56509f7c23d495fa81dc07b5ed38419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14fdf26fefb4453381b45dd79a53b42f"
      ],
      "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
     }
    },
    "b2492a07ae4649f29c0d0a8c23c4047e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "c7ba7e7ad7a1478498bd1b7035cfa262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [
       "button2"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Browser login",
      "disabled": true,
      "icon": "",
      "layout": "IPY_MODEL_0e7d0228b5b24d0ba23a493264adb815",
      "style": "IPY_MODEL_f75a1fb831f64ca59dfeb7ed8e411483",
      "tooltip": ""
     }
    },
    "c8429c73d5554f7e9f6578a7c04557af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdd962679dbb49e3873289ef6bd13633": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d22020b9cdcb4db5b903ec0510d77bf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "",
      "description_tooltip": null,
      "disabled": true,
      "layout": "IPY_MODEL_cdd962679dbb49e3873289ef6bd13633",
      "placeholder": "Personal Access Token (PAT)",
      "style": "IPY_MODEL_f4b7f904482d4e5d9ab526fe7897f256",
      "value": ""
     }
    },
    "da795a9e9c8c47c489b520483c95713a": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_f33b28cfdc71491baa49f843b1f51389",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> <span style=\"font-weight: bold\">Downloading</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">950009856/953017547</span> • <span style=\"color: #800000; text-decoration-color: #800000\">15933119 QPS</span> • <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> • <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">953.0 MB</span>\n</pre>\n",
         "text/plain": "\u001b[32m⠋\u001b[0m \u001b[1mDownloading\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m950009856/953017547\u001b[0m • \u001b[31m15933119 QPS\u001b[0m • \u001b[36m0:00:01\u001b[0m • \u001b[1;34m953.0 MB\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "eedc32bcc61a430dadec9bca5a508745": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f27061fd69d24cd6a77a694aec8906f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "300px"
     }
    },
    "f33b28cfdc71491baa49f843b1f51389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4b7f904482d4e5d9ab526fe7897f256": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f75a1fb831f64ca59dfeb7ed8e411483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
