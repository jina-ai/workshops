{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui5pz-xNNoAR"
      },
      "source": [
        "# Search PDF text, images and tables with Python and CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7u3W0lL6db"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glXcJ4rAF0Gb"
      },
      "source": [
        "Have you ever been searching through a stack of files and just can't find the right keywords to get what you're looking for? Staring at a screen and wracking your brain at 3am for the right word ain't fun, take it from me.\n",
        "\n",
        "How about trying to search through a stack of PDFs? That gets even harder since all that nice plain text is wrapped up in [a gnarly format](https://forum.quartertothree.com/t/is-pdf-an-evil-format/58598). Good luck grepping those!\n",
        "\n",
        "And what if you want to search **tables and images** as well as text? \n",
        "\n",
        "In this notebook we're going to kill those three birds with one stone.\n",
        "\n",
        "We'll harness the power of AI to find things *similar* to the search query you input, and we'll show you how to deploy that search engine in real life for anyone to use.\n",
        "\n",
        "We're going to do this with open-source tools from the Jina ecosystem.\n",
        "\n",
        "### Why Jina and [neural search](https://docs.jina.ai/get-started/neural-search?utm_source=pdf-notebook)? What's wrong with good old symbolic search?\n",
        "\n",
        "#### Semantics semantics semantics!\n",
        "\n",
        "Instead of just matching patterns, our search engine will match *meanings*. So if we were to search [`arthropod`](https://examples.yourdictionary.com/examples-of-arthropods.html), our top results would be related directly to arthropods, but we'd also get results for spiders, scorpions, horseshoe crabs and lots of other cute related critters. This is because we're using deep neural nets (DNNs) to embed words in a vector space so that words with similar meanings have similar [embeddings](https://docarray.jina.ai/fundamentals/document/embedding?utm_source=pdf-notebook).\n",
        "\n",
        "*An example of an arthropod, specifically a Trilobite:*\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Estonian_Museum_of_Natural_History_-_trilobite_-_Hydrocephalus.png/1280px-Estonian_Museum_of_Natural_History_-_trilobite_-_Hydrocephalus.png)\n",
        "\n",
        "#### Less code to write\n",
        "\n",
        "Using Jina Hub, we reduce the amount of code we need to write. Instead of  integrating [Transformers](https://hub.jina.ai/executor/u9pqs8eb) with our search engine, we can simply use a couple of lines of code to download it from [Jina Hub](https://hub.jina.ai), run it in Docker, or run it in a [sandbox](https://docs.jina.ai/how-to/sandbox?utm_source=pdf-notebook) on the cloud. And if we wanted to swap it out for something like [spaCy](https://hub.jina.ai/executor/u7h7cuh2)? Again, just a matter of changing a couple of lines of code.\n",
        "\n",
        "#### Deployment made easy\n",
        "\n",
        "Also, tools like Jina take a lot of hassle out of the orchestration and scaling. We can easily add [sharding, replicas](https://docs.jina.ai/how-to/scale-out/?highlight=sharding), [Kubernetes integration](https://docs.jina.ai/how-to/kubernetes?utm_source=pdf-notebook), and so on. \n",
        "\n",
        "### Meet our ingredients\n",
        "\n",
        "#### **[DocArray](https://docarray.jina.ai?utm_source=pdf-notebook)**\n",
        "\n",
        "DocArray is a library for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the multi-modal data with a Pythonic API. ([star the repo]())\n",
        "\n",
        "#### **[Jina](https://docs.jina.ai)**\n",
        " \n",
        " Jina is a framework that empowers anyone to build cross-modal and multi-modal[*] applications on the cloud. It uplifts a PoC into a production-ready service. Jina handles the infrastructure complexity, making advanced solution engineering and cloud-native technologies accessible to every developer. ([star the repo]())\n",
        "\n",
        "#### **[Jina Hub](https://hub.jina.ai)**\n",
        "\n",
        "Download pre-built building blocks for neural search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZQBqev0Fleb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r0DglZWL9oG"
      },
      "outputs": [],
      "source": [
        "!pip install -q docarray[full]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFnRxjbFleb"
      },
      "outputs": [],
      "source": [
        "!pip install -q ipywidgets jina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZVcFkrIFlec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OQ6BENqFled"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')  # ignore all those pesky warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgYFyepOchY"
      },
      "source": [
        "## Downloading our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPfdmIfaMcCT"
      },
      "source": [
        "We're using a couple of PDFs downloaded from arxiv.org. Of course, this is just a toy dataset. PDFs can differ in many ways, and depending on your use case you may need to process them very differently (e.g. OCR, image processing). Since ours are simple plain text, these steps will apply to most PDF search engines you may wish to build.\n",
        "\n",
        "I selected these PDFs because they included images, text and tables, and extracting/processing those is a key part of this notebook.\n",
        "\n",
        "---\n",
        "\n",
        "#### âš™ï¸ Want to use your  own data?\n",
        "\n",
        "In that case:\n",
        "\n",
        "* Ignore the cell below\n",
        "* Create a `data` directory in the \"Files\" sidebar\n",
        "* Copy your own PDFs into that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y9HT-Rddsfy"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(\"data\"):\n",
        "  !wget -q -N --output-document data.zip https://github.com/jina-ai/workshops/blob/main/notebooks/pdf_search/part_2_images_and_text/data.zip?raw=true\n",
        "  !unzip -n data.zip\n",
        "  !rm -f data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZGCBbx2PF9S"
      },
      "source": [
        "## Loading our PDF files\n",
        "\n",
        "We'll use a [DocumentArray](https://docarray.jina.ai/fundamentals/documentarray/) from the [DocArray](https://docarray.jina.ai/) package to collect all of our PDFs, then [load them as binary blob data](https://docarray.jina.ai/fundamentals/document/fluent-interface/#blobdata/) into [Document](https://docarray.jina.ai/fundamentals/document/) instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osL261L3dhd2"
      },
      "outputs": [],
      "source": [
        "from docarray import DocumentArray, Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AMszuh7iE0t"
      },
      "outputs": [],
      "source": [
        "docs = DocumentArray.from_files(\"data/*.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt4kS0DdkxGf"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "  doc.load_uri_to_blob()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GfxgpWCQKMJ"
      },
      "source": [
        "## Creating a Flow\n",
        "\n",
        "We'll use Jina to generate [Flows](https://docs.jina.ai/fundamentals/flow?utm_source=pdf-notebook) for indexing and searching. Our Documents will pass through these when we're indexing or searching.\n",
        "\n",
        "A Flow is built out of [Executors](https://docs.jina.ai/fundamentals/executor?utm_source=pdf-notebook), each of which perform a single processing task on each Document. We'll use [Jina Hub]() to provide pre-made Executors, meaning we don't have to write so much code.\n",
        "\n",
        "Compared to our [previous PDF search engine](https://colab.research.google.com/github/jina-ai/workshops/blob/main/pdf_search/pdf_search.ipynb), this Flow has a lot more Executors. You can read about them in our blog post.\n",
        "\n",
        "### Why just one Flow?\n",
        "\n",
        "In a later notebook we'll deploy and host our Flow on [JCloud](https://docs.jina.ai/fundamentals/jcloud/) for free. This requires us to use just one Flow for both indexing and searching.\n",
        "\n",
        "### Why is this Flow so complex?\n",
        "\n",
        "Using one Flow to handle both indexing and searching means:\n",
        "\n",
        "- When we submit a search term it's merely a text string or an image, both of which we wrap in a Document. We don't need to extract them from any other kind of data, so we can skip a lot of Executors (anything with the name prefix of `index_`)\n",
        "- Our search Document is a \"root-level\" Document - i.e. the content is right at the \"top\". Our indexed Documents are at chunk-level (the sentences, images, and tables extracted from the top-level PDF). So we need to use Executors with different `traversal_paths`. This means duplicating a few Executors, with one to run during indexing (prefixed `index_`) and one during searching (`search_`).\n",
        "- Some Executors (like [AnnLiteIndexer](https://hub.jina.ai/executor/7yypg8qk)) are used both for indexing and searching, so are prefixed `all_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDdrm1ezGaaT"
      },
      "outputs": [],
      "source": [
        "from jina import Flow, Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3wCflQkT7_b"
      },
      "outputs": [],
      "source": [
        "flow = (\n",
        "    Flow()\n",
        "    .add(\n",
        "        uses=\"jinahub://PDFTableExtractor/latest\", # Extract tables\n",
        "        install_requirements=True,\n",
        "        name=\"index_table_extractor\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://PDFSegmenter\", # Extract images/text\n",
        "        install_requirements=True,\n",
        "        name=\"index_segmenter\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://ElementTypeTagger\", # Tag Documents based on modality (image/text/table)\n",
        "        uses_with={\"traversal_paths\": \"@c\"},\n",
        "        name=\"index_tagger\",\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://SpacySentencizer\", # Sentencize long text into sentences\n",
        "        uses_with={\"traversal_paths\": \"@c\"},\n",
        "        install_requirements=True,\n",
        "        name=\"index_sentencizer\",\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://TagsCopier\", # Recursively copy tags\n",
        "        uses_with={\"traversal_paths\": \"@c\"},\n",
        "        name=\"index_tags_copier\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://ChunkFlattener\", # Flatten all chunks to doc.chunks\n",
        "        name=\"index_flattener\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://ImagePreprocessor-skip-non-images\", # Process images in PDF chunks\n",
        "        uses_with={\"traversal_paths\": \"@c\"},\n",
        "        install_requirements=True,\n",
        "        name=\"index_image_processor\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://ImagePreprocessor-skip-non-images\", # Process search query image\n",
        "        uses_with={\"traversal_paths\": \"@r\"},\n",
        "        install_requirements=True,\n",
        "        name=\"search_image_processor\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://CLIPEncoder/latest-gpu\", # Encode using CLIP - chunk level\n",
        "        uses_with={\"traversal_paths\": \"@c\"},\n",
        "        install_requirements=True,\n",
        "        name=\"index_encoder\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://CLIPEncoder/latest-gpu\", # Encode using CLIP - root level\n",
        "        install_requirements=True,\n",
        "        name=\"search_encoder\"\n",
        "    )\n",
        "    .add(\n",
        "        uses=\"jinahub://AnnLiteIndexer\", # Store vectors and metadata on disk\n",
        "        uses_with={\n",
        "            \"index_traversal_paths\": \"@c\",\n",
        "            \"search_traversal_paths\": \"@c\",\n",
        "            \"columns\": [(\"element_type\", \"str\")],\n",
        "            \"n_dim\": 512\n",
        "            },\n",
        "        install_requirements=True,\n",
        "        name=\"all_indexer\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2basx4HcC2r"
      },
      "outputs": [],
      "source": [
        "flow.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REW6LCKbQX0b"
      },
      "source": [
        "## Indexing our Documents\n",
        "\n",
        "Now it's time to run the Flow.\n",
        "\n",
        "First we'll remove any old index data that may be lying around to ensure nothing carried over from a prior run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y15-4H5P2hn"
      },
      "outputs": [],
      "source": [
        "!rm -rf workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFgzwBZlt7lF"
      },
      "source": [
        "ðŸš¨ **Note:** if the below cell fails, restart the runtime (*Runtime* > *Restart runtime*) and run all the cells again. This seems to be an issue with Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcNUF796LxKb"
      },
      "outputs": [],
      "source": [
        "with flow:\n",
        "  client = Client(port=flow.port)\n",
        "  docs = client.post(\"/index\", docs, request_size=1, show_progress=True, target_executor=\"(index_*|all_*)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a39tLeqOYwAj"
      },
      "source": [
        "### Examining our Documents\n",
        "\n",
        "Now that we've done all that processing, what do our Documents look like?\n",
        "\n",
        "Let's look at the indexed DocumentArray to start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLttVdotY4Kl"
      },
      "outputs": [],
      "source": [
        "docs.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlFwkPASY6HU"
      },
      "source": [
        "And now the first Document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWZT7bFbY-C_"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpVHAfNNaYqg"
      },
      "outputs": [],
      "source": [
        "docs[0].chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_2Lh5_oerFw"
      },
      "source": [
        "Here's a chunk with its embedding and tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPAKSLN-eQLd"
      },
      "outputs": [],
      "source": [
        "docs[0].chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6RyADoBVASl"
      },
      "source": [
        "## Searching our data\n",
        "\n",
        "For performing a search, we need to:\n",
        "\n",
        "- Create a Document containing our search query (either image or text)\n",
        "- If the search query is an image, convert to a tensor so CLIPEncoder can read it\n",
        "- Encode the search query with CLIPEncoder\n",
        "- Search through the already indexed data with the search query\n",
        "\n",
        "You can also specify filters for `element_type` (either `text`, `table`, or `image`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw0QUTi4T9Nx"
      },
      "outputs": [],
      "source": [
        "search_format = \"text\" # text or image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZOdtBtnUv87"
      },
      "source": [
        "### Using a text search term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeWUqFoaUqb1"
      },
      "outputs": [],
      "source": [
        "if search_format == \"text\":\n",
        "  search_term = \"trilobite diagram\"\n",
        "  query_doc = Document(text=search_term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nyh-ci-UydF"
      },
      "source": [
        "### Using an image search term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLQUpGzU4Rx"
      },
      "outputs": [],
      "source": [
        "if search_format == \"image\":\n",
        "  # Download image\n",
        "  image_url = \"http://paleonet.org/TTP/files/stacks-image-f0024aa.jpg\"\n",
        "  !wget -q --output-document image.png $image_url\n",
        "\n",
        "  query_doc = Document(uri=\"image.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT0ec6IcU1U7"
      },
      "source": [
        "### Applying search filter\n",
        "\n",
        "[AnnLiteIndexer](https://hub.jina.ai/executor/7yypg8qk?utm_source=notebook-pdf-search-tables) allows you to apply MongoDB-style filters. Check the [Executor's README](https://hub.jina.ai/executor/7yypg8qk) to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QaxwaoEWiJI"
      },
      "outputs": [],
      "source": [
        "# you can use any combination of text/table/image\n",
        "\n",
        "element_type = [\n",
        "    \"text\", \n",
        "    \"image\" \n",
        "    \"table\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcsC3zxbv_GI"
      },
      "outputs": [],
      "source": [
        "filter = {\n",
        "    \"element_type\": {\n",
        "        \"$in\": element_type,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Dd3iu0Wj6E"
      },
      "source": [
        "### Performing the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTqRVpNMi3ej"
      },
      "outputs": [],
      "source": [
        "with flow:\n",
        "  client = Client(port=flow.port)\n",
        "\n",
        "  results = client.post(\n",
        "      \"/search\",\n",
        "      query_doc, \n",
        "      request_size=1,\n",
        "      parameters={\n",
        "          \"filter\": filter\n",
        "      },\n",
        "      show_progress=True, \n",
        "      target_executor=\"(search_*|all_*)\"\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz-pJa7xQtVQ"
      },
      "source": [
        "### Show results\n",
        "\n",
        "If the results are text or table, just print it out. Otherwise we can plot the image matches in the notebook.\n",
        "\n",
        "Note: Due to the content of the PDFs, *most* results will be text results. You can change the `filter` above to select instead for tables and/or images.\n",
        "\n",
        "The `render()` function below is needed to render the search results in a notebook. In the real world you'd probably want to do something different, but this quick, hacky code (specifically tailored for notebooks, not real world) will serve for now."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def render(docarray):\n",
        "  for idx, doc in enumerate(docarray):\n",
        "    if doc.tags[\"element_type\"] == \"image\":\n",
        "      os.makedirs(\"images\", exist_ok=True)\n",
        "      filename = f\"images/{idx}-{doc.id}.png\"\n",
        "      doc.set_image_tensor_inv_normalization(channel_axis=0)\n",
        "      doc.save_image_tensor_to_file(filename, channel_axis=0)\n",
        "      image=plt.imread(filename)\n",
        "      fig=plt.figure()\n",
        "      plt.axis('off')\n",
        "      plt.imshow(image)\n",
        "\n",
        "    elif doc.tags[\"element_type\"] == \"table\":\n",
        "      os.makedirs(\"csvs\", exist_ok=True)\n",
        "      filename = f\"csvs/{idx}-{doc.id}.csv\" \n",
        "      with open(filename, \"w\") as file:\n",
        "        file.write(doc.tags[\"table_content\"])\n",
        "      df = pd.read_csv(filename)\n",
        "      print(df)\n",
        "      \n",
        "    else:\n",
        "      print(doc.text)"
      ],
      "metadata": {
        "id": "k37pMvD53vha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render(results[0].matches)"
      ],
      "metadata": {
        "id": "SlGUqYcl6xyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVQ-AHbSuZRi"
      },
      "source": [
        "## Putting it into production\n",
        "\n",
        "Colab notebooks have a number of restrictions that make real-world stuff quite difficult. If we were building this outside of a notebook, we could:\n",
        "\n",
        "* Set up a [RESTful or gRPC gateway](https://docs.jina.ai/fundamentals/gateway?utm_source=pdf-notebook) and keep the Flow open to requests using `flow.block()`\n",
        "* Use [sharding and replicas](https://docs.jina.ai/how-to/scale-out?utm_source=pdf-notebook) to improve performance and reliability.\n",
        "* [Monitor our Flow with Grafana](https://docs.jina.ai/fundamentals/flow/monitoring-flow?utm_source=pdf-notebook)\n",
        "* Better yet, host our Flow on [JCloud](https://docs.jina.ai/fundamentals/jcloud?utm_source=pdf-notebook), so we don't have to use any of our own compute for encoding, indexing, hosting, etc (encoding is especially hungry on the hardware)\n",
        "* Finetune our results using [Finetuner](https://finetuner.jina.ai) to provide better matches\n",
        "* Use a more specialized model (rather than just general purpose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrGzDkrQRBwQ"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### No text is being extracted from my PDF\n",
        "\n",
        "It might be that your PDF is full of *pictures of text* rather than text itself. This is quite common. In a future notebook we'll integrate an OCR Executor like [PaddlePaddleOCR](https://hub.jina.ai/executor/78yp7etm) to get around this.\n",
        "\n",
        "### I'm getting bad search results in my language\n",
        "\n",
        "The CLIP model we're using is trained primarily on English. Multilingual CLIP models do exist however. You can define which model you want to use with the `pretrained_model_name_or_path` argument in [CLIPEcoder](https://hub.jina.ai/executor/29r2b26t).\n",
        "\n",
        "### My tables aren't being extracted\n",
        "\n",
        "The docs2info's table extraction service is still being tested. While it's provided good results in my experience, it's still under heavy development.\n",
        "\n",
        "### The notebook fails when I do anything involving images\n",
        "\n",
        "Try restarting the runtime (there should be an option for that near the top, under the `!pip install docarray[full]` cell. This seems to be a notebook limitation.\n",
        "\n",
        "### It's too slow!\n",
        "\n",
        "Have you enabled Colab's GPU under *Runtime* > *Change runtime type*?\n",
        "\n",
        "### Something else?\n",
        "\n",
        "Join our [Slack](https://slack.jina.ai) and ask us there in the #projects-pdf channel!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktOVGnmKuRx2"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "Want to dig more into the Jina ecosystem? Here are some resources:\n",
        "\n",
        "- [Developer portal](https://learn.jina.ai) - tutorials, courses, videos on using Jina\n",
        "- [Fashion search notebook](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb) - build an image-to-image fashion search engine\n",
        "- [DALL-E Flow](https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb#scrollTo=NeWDy9viOCAP)/[Disco Art](https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb#scrollTo=47428f37) - create AI-generated art in your browser"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8ZQBqev0Fleb",
        "fGgYFyepOchY"
      ],
      "name": "pdf_search_2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}