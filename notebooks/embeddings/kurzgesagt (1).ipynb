{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt3nai8YENmY"
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVSBQ3JSiHR-",
    "outputId": "ef563840-6c61-441b-9c2c-f4c3b8a45f13"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy faiss-cpu sagemaker requests jina-sagemaker tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwysaiFRRE-y"
   },
   "source": [
    "# Set up Jina Embeddings v2 on Sagemaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYVZlnmWt84Y"
   },
   "source": [
    "## Install dependencies and configure AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewBIrG7BSPZj",
    "outputId": "12090e38-d36f-41b6-c7f5-69551012a1a6"
   },
   "outputs": [],
   "source": [
    "!pip install aws configure\n",
    "!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucwSSF2oS8lT",
    "outputId": "bbe35a58-30fb-48cf-a358-f6759651f6e3"
   },
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rji2nLwj4kBL"
   },
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::253352124568:role/service-role/AmazonSageMaker-ExecutionRole-20230527T104084\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "#role = boto3.Session().role\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJV5o5W6uE4i"
   },
   "source": [
    "# Connect to Jina Embeddings v2 Endpoint on Sagemaker\n",
    "\n",
    "To have this working you should have first already created an endpoint for inference, by\n",
    "1. Subscribing to [_Jina Embeddings v2 Base - en_](https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w) package on AWS marketplace.\n",
    "2. Creating a [Sagemaker Endpoint](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints) for inference using the subscribed model.\n",
    "\n",
    "Once an endpoint is created, continue running the code-blocks below to deploy the created endpoints, and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4in-3gdZv7y"
   },
   "outputs": [],
   "source": [
    "# Specify the name of the model that you subscribed to\n",
    "subscribed_model_name = \"jina-embeddings-v2-base-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJYyYcfsRLi4"
   },
   "outputs": [],
   "source": [
    "# Mapping for Model Package Names\n",
    "model_name_map = {\n",
    "    \"jina-embeddings-v2-base-en\": \"jina-embeddings-v2-base-en-32555da8a0b431d190bf3eca46758b72\",\n",
    "    \"jina-embeddings-v2-small-en\": \"jina-embeddings-v2-small-en-0e950fb984e3396fa4e1108adf69937c\",\n",
    "    \"jina-embeddings-v2-base-de\": \"jina-embeddings-v2-base-de-c269d166764133348365f57b8f1d8c7a\",\n",
    "    \"jina-embeddings-v2-base-zh\": \"jina-embeddings-v2-base-zh-4da30f467aaf347580ba5ed2648e399a\",\n",
    "}\n",
    "\n",
    "# Specify the model name\n",
    "model_name = model_name_map[subscribed_model_name]\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n",
    "}\n",
    "\n",
    "# Specify the model you want to use\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Maxsey4rTSib",
    "outputId": "ef178306-2da1-4416-9a0b-af7ef47a52bb"
   },
   "outputs": [],
   "source": [
    "from jina_sagemaker import Client\n",
    "\n",
    "client = Client(region_name=region)\n",
    "\n",
    "# Choose the name of the endpoint that you created after subscribing to the model\n",
    "endpoint_name = \"jina-embeddings-v2-base-en\"\n",
    "\n",
    "client.create_endpoint(\n",
    "    arn=model_package_arn,\n",
    "    role=role,\n",
    "    endpoint_name=endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    n_instances=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-By86zDzN7Cl"
   },
   "source": [
    "# Index Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUrApshlunsW"
   },
   "outputs": [],
   "source": [
    "client.connect_to_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZV6VG2eAIl0"
   },
   "source": [
    "In this RAG tutorial, we will use a [dataset](https://www.kaggle.com/datasets/maartengr/kurzgesagt-transcriptions?resource=download) (CC0 licenced) containing transcripts of videos from a popular YouTube channel, called [_Kurzgesagt_](https://www.youtube.com/channel/UCsXVk37bltHxD1rDPwtNM8Q) (\"_In a Nutshell_\" in English).\n",
    "\n",
    "The dataset contains, in each row, the title of a video, its url, and then transcript of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "073agqr3AMTc"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i63K8oAi8Qd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('https://drive.google.com/uc?export=download&id=18FO21nrfkQ1Vuee2g_773ON2256nOrlS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbq308Vg7UCs"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A69VJ6AVAPP0"
   },
   "source": [
    "## Chunk and embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e1MuouGjeKK",
    "outputId": "c2466127-3d5b-42ae-fb89-27fcceca0de6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def chunk_text(text, max_words=128):\n",
    "    \"\"\"\n",
    "    Divide text into chunks where each chunk contains the maximum number of full sentences under `max_words`.\n",
    "    \"\"\"\n",
    "    sentences = text.split('.')\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip(\".\")\n",
    "        if not sentence:\n",
    "          continue\n",
    "\n",
    "        words_in_sentence = len(sentence.split())\n",
    "        if word_count + words_in_sentence <= max_words:\n",
    "            chunk.append(sentence)\n",
    "            word_count += words_in_sentence\n",
    "        else:\n",
    "            # Yield the current chunk and start a new one\n",
    "            if chunk:\n",
    "              yield '. '.join(chunk).strip() + '.'\n",
    "            chunk = [sentence]\n",
    "            word_count = words_in_sentence\n",
    "\n",
    "    # Yield the last chunk if it's not empty\n",
    "    if chunk:\n",
    "        yield ' '.join(chunk).strip() + '.'\n",
    "\n",
    "def generate_embeddings(text_df):\n",
    "    chunks = list(chunk_text(text_df['Text']))\n",
    "    embeddings = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "      response = client.embed(texts=[chunk])\n",
    "      chunk_embedding = response[0]['embedding']\n",
    "      embeddings.append(np.array(chunk_embedding))\n",
    "\n",
    "    text_df['chunks'] = chunks\n",
    "    text_df['embeddings'] = embeddings\n",
    "    return text_df\n",
    "\n",
    "print(\"Embedding text chunks ...\")\n",
    "df = df.progress_apply(generate_embeddings, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aDhBP_17vAU"
   },
   "source": [
    "# Set up Semantic Search using Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82dUlOzNCfwY"
   },
   "source": [
    "We will now use [Faiss](https://github.com/facebookresearch/faiss), an in-memory vector similarity search engine, to find the most similar chunks to a given query.\n",
    "\n",
    "We first need to create an index and add the embeddings to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oqdvmig58Oi5"
   },
   "source": [
    "## Indexing vectors in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8IWwf92k8KA"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = 768  # dimension of the embeddings\n",
    "index_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
    "k = 0\n",
    "\n",
    "doc_ref = dict()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    embeddings = row['embeddings']\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype='float32').reshape(1, -1))\n",
    "        faiss.normalize_L2(normalized_embedding)\n",
    "        index_with_ids.add_with_ids(normalized_embedding, k)\n",
    "        doc_ref[k] = (row['chunks'][i], idx)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE7qbTpv8SX6"
   },
   "source": [
    "## Vector search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_SLKBb8-p-I"
   },
   "outputs": [],
   "source": [
    "def find_most_similar_transcript_segment(query, n=3):\n",
    "    query_embedding = client.embed(texts=[query])[0]['embedding']  # Assuming the query is short enough to not need chunking\n",
    "    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype='float32').reshape(1, -1))\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    D, I = index_with_ids.search(query_embedding, n)  # Get the top n matches\n",
    "\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        distance = D[0][i]\n",
    "        index_id = I[0][i]\n",
    "        transcript_segment, doc_idx = doc_ref[index_id]\n",
    "        results.append((transcript_segment, doc_idx, distance))\n",
    "\n",
    "    # Sort the results by distance\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym1zl1HJDjd9"
   },
   "source": [
    "# Set up Question-answering using LLM on Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zxcfZWFszfz"
   },
   "source": [
    "Note that this step may take some time approx. 5-10 minutes while deploying an endpoint on AWS Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MiTRSHj8eY4A",
    "outputId": "3355bf7d-79d6-4b51-d497-c3700a5eb2b0"
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "jumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\n",
    "model_predictor = jumpstart_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxQRx35BAfL2"
   },
   "source": [
    "# Putting it all together for RAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sn_oWaFDrAt"
   },
   "source": [
    "## Query the vector search engine with your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tlvqezIEVHY",
    "outputId": "9b8f4faf-ec9d-4aff-f5ea-d16cbeb12c05"
   },
   "outputs": [],
   "source": [
    "question = input(\"Enter your question: \")\n",
    "search_results = find_most_similar_transcript_segment(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NS-dQ2H27OT9",
    "outputId": "96f40cc5-1bab-4fb7-b627-7839e088617d"
   },
   "outputs": [],
   "source": [
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuXpASfKDl7C"
   },
   "source": [
    "## Templating a prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5p1_KPFEtC5"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt_template = Template(\"\"\"\n",
    "  <s>[INST] Answer the question below only using the given context.\n",
    "  The question from the user is based on transcripts of videos from a YouTube\n",
    "    channel.\n",
    "  The context is presented as a ranked list of information in the form of\n",
    "    (video-title, transcript-segment), that is relevant for answering the\n",
    "    user's question.\n",
    "  The answer should only use the presented context. If the question cannot be\n",
    "    answered based on the context, say so.\n",
    "\n",
    "  Context:\n",
    "  1. Video-title: $title_1, transcript-segment: $segment_1\n",
    "  2. Video-title: $title_2, transcript-segment: $segment_2\n",
    "  3. Video-title: $title_3, transcript-segment: $segment_3\n",
    "\n",
    "  Question: $question\n",
    "\n",
    "  Answer: [/INST]\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "prompt_for_llm = prompt_template.substitute(\n",
    "    question = question,\n",
    "    title_1 = df.iloc[search_results[0][1]][\"Title\"].strip(),\n",
    "    segment_1 = search_results[0][0],\n",
    "    title_2 = df.iloc[search_results[1][1]][\"Title\"].strip(),\n",
    "    segment_2 = search_results[1][0],\n",
    "    title_3 = df.iloc[search_results[2][1]][\"Title\"].strip(),\n",
    "    segment_3 = search_results[2][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDL_XomJ0Cra",
    "outputId": "cc60a512-7375-475c-f202-3b59efe085f3"
   },
   "outputs": [],
   "source": [
    "print(prompt_for_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw1VK0dGD0TR"
   },
   "source": [
    "## Send prompt to LLM and print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldpEjxfJ-BmB",
    "outputId": "89a5bf7c-df07-4ea8-859c-a6b042e8ca8d"
   },
   "outputs": [],
   "source": [
    "payload = {\"inputs\": prompt_for_llm}\n",
    "model_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgEYvaErWyKE"
   },
   "source": [
    "# Clean up all Sagemaker Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S01M0hh7wiIX"
   },
   "source": [
    "It's super important to do this *EVERY TIME* you deploy an endpoint!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXKvMt5eW1al"
   },
   "outputs": [],
   "source": [
    "client.delete_endpoint()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlWTzaefgiba"
   },
   "outputs": [],
   "source": [
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1oWUsOtwmtH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
