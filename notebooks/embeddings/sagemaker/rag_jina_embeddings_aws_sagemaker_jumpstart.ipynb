{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4de412-9337-464a-8f75-b7cda4c6019d",
   "metadata": {},
   "source": [
    "# Deploy Jina Models on AWS SageMaker\n",
    "\n",
    "This notebook was created with the **Data Science 3.0 image** on **ml.t3.medium** instance on SageMaker Studio\n",
    "\n",
    "[Jina Embeddings](https://jina.ai/embeddings/) and [Jina Reranker](https://jina.ai/reranker/) are now available to use with [SageMaker](https://aws.amazon.com/pm/sagemaker/) from the [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy). \n",
    "\n",
    "This notebook walks you through creating a [Retrieval-augmented generation (RAG)](https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/) application in AWS SageMaker for a collection of YouTube video transcripts. The models we will use are Jina Embeddings v2 - English, Jina Reranker v1, and the [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) large language model, all of which are available to SageMaker users.\n",
    "\n",
    "You will need to have an AWS account. If you are not already an AWS user, you can [sign up for an account](https://portal.aws.amazon.com/billing/signup) on the AWS website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633db504-5a28-4f9e-93ce-804cba226c20",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Install the jina-sagemaker package and additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759da6d-329b-45b2-b5fb-670d7b5f4bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " !pip install sagemaker jina-sagemaker setuptools  --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728653f-a192-4f30-8f9d-a00449aa141d",
   "metadata": {},
   "source": [
    "## Configure a Role\n",
    "\n",
    "You will need an [AWS role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) with sufficient permissions to use the resources required for this tutorial. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d46d8-b41c-4f2f-a613-0e41da59f60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "role_name = role.split([\"/\"][-1])\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "print(f\"The Amazon Resource Name (ARN) of the role used for this demo is: {role}\")\n",
    "print(f\"The name of the role used for this demo is: {role_name[-1]}\")\n",
    "print(f\"The default region is: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c77be-6334-40ca-be49-130dc212c11a",
   "metadata": {},
   "source": [
    "To verify that the role above has required permissions for this tutorial:\n",
    "\n",
    "1. Go to the IAM console: https://console.aws.amazon.com/iam/home.\n",
    "2. Select **Roles**.\n",
    "3. Enter the role name in the search box to search for that role. \n",
    "4. Select the role.\n",
    "5. Use the **Permissions** tab to verify this role has required permissions below attached:\n",
    "    \n",
    "        1. aws-marketplace:ViewSubscriptions\n",
    "        2. aws-marketplace:Unsubscribe\n",
    "        3. aws-marketplace:Subscribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0520f-b288-42be-923c-94d4a7cabe05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Subscribe to Jina AI Models on AWS Marketplace\n",
    "\n",
    "Subscribe to the [Jina Embeddings v2 base English](https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w) and [Jina Reranker v1 ](https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6).\n",
    "\n",
    "When you’ve subscribed to them, we get the models’ ARNs for your AWS region and store them in the variable names `embedding_package_arn` and `reranker_package_arn` respectively. The code in this tutorial will reference them using those variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a770f9d-9fad-4707-ad8f-0d7402f5225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_arn_for_model(region, model_name):\n",
    "    model_package_map = {\n",
    "        \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n",
    "        \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n",
    "        \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n",
    "        \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n",
    "        \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n",
    "        \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n",
    "        \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n",
    "        \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n",
    "        \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n",
    "        \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n",
    "        \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n",
    "        \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n",
    "        \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n",
    "        \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n",
    "        \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n",
    "        \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n",
    "    }\n",
    "\n",
    "    return model_package_map[region]\n",
    "\n",
    "embedding_package_arn = get_arn_for_model(region, \"jina-embeddings-v2-base-en\")\n",
    "reranker_package_arn = get_arn_for_model(region, \"jina-reranker-v1-base-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30971640-b5fb-4c43-922b-57f0259104da",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "In this tutorial, we are going to use a collection of videos provided by the YouTube channel [TU Delft Online Learning](https://www.youtube.com/@tudelftonlinelearning1226). This channel produces a variety of educational materials in STEM subjects. Its programming is [CC-BY licensed](https://creativecommons.org/licenses/by/3.0/legalcode).\n",
    "\n",
    "We downloaded 193 videos from the channel and processed them with OpenAI’s open-source [Whisper speech recognition model](https://openai.com/research/whisper). We used the smallest model ([`openai/whisper-tiny`](https://huggingface.co/openai/whisper-tiny) [on Hugging Face](https://huggingface.co/openai/whisper-tiny)) to process the videos into transcripts. \n",
    "\n",
    "The transcripts have been organized into a CSV file, which you can [download from here](https://tbd.todo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42add80-4d60-4c07-9142-8ed86915e2bb",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444757c1-d349-4a52-9b73-34352c2ba8bd",
   "metadata": {},
   "source": [
    "This data is CSV format and will be handled using `pandas` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fcea2-6826-4e04-8ff6-1ef222c6e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb11eff-61e4-4180-8d85-58510f1361ed",
   "metadata": {},
   "source": [
    "## Download the Data into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fb813-043a-43e8-af5b-ed9921137b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Load the CSV file\n",
    "data_url = \"https://github.com/jina-ai/workshops/raw/main/notebooks/embeddings/sagemaker/tu_delft.csv\"\n",
    "tu_delft_dataframe = pandas.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d3f88-ddd5-4237-8b79-bac381fd46d8",
   "metadata": {},
   "source": [
    "Run the line below to inspect the first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc345a8-e333-43e6-82c8-d676dc162c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tu_delft_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce39b2-a45c-4c64-937a-7f5e6249973d",
   "metadata": {},
   "source": [
    "# Start the Jina Embeddings v2 Endpoint\n",
    "\n",
    "The code below will launch an instance of `ml.g4dn.xlarge` on AWS to run the embedding model.\n",
    "\n",
    "It may take several minutes for this to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc13151-cf27-4661-8044-42379271d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from jina_sagemaker import Client\n",
    "\n",
    "# Choose a name for your embedding endpoint. It can be anything convenient.\n",
    "embeddings_endpoint_name = \"jina_embedding\"\n",
    "\n",
    "embedding_client = Client(region_name=region)\n",
    "embedding_client.create_endpoint(\n",
    "    arn=embedding_package_arn,\n",
    "    role=role,\n",
    "    endpoint_name=embeddings_endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    n_instances=1,\n",
    ")\n",
    "\n",
    "embedding_client.connect_to_endpoint(endpoint_name=embeddings_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b43ba3-066b-4448-aee8-e151a024fb4d",
   "metadata": {},
   "source": [
    "# Build and Index the Dataset\n",
    "\n",
    "Now that we have loaded the data and are running a Jina Embeddings v2 model, we can prepare and index the data. We will store the data in a [FAISS vector store](https://faiss.ai/index.html), an open-source vector database designed specifically for AI applications.\n",
    "\n",
    "First, install the remaining prerequisites for our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f00a79-4d24-4c21-bb5f-5bf159d38ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tdqm numpy faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfd968-c351-4ae6-b4ad-e3feea546c17",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "We will need to take the individual transcripts and split them up into smaller parts, i.e., “chunks” so that we can fit multiple texts into a prompt for the LLM. The code below breaks the individual transcripts up on sentence boundaries, ensuring that all chunks have no more than 128 words by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232d3de-0507-4e1d-8b62-c738f83fce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=128):\n",
    "    \"\"\"\n",
    "    Divide text into chunks where each chunk contains the maximum number of full \n",
    "    sentences under `max_words`.\n",
    "    \"\"\"\n",
    "    sentences = text.split('.')\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip(\".\")\n",
    "        if not sentence:\n",
    "          continue\n",
    "\n",
    "        words_in_sentence = len(sentence.split())\n",
    "        if word_count + words_in_sentence <= max_words:\n",
    "            chunk.append(sentence)\n",
    "            word_count += words_in_sentence\n",
    "        else:\n",
    "            # Yield the current chunk and start a new one\n",
    "            if chunk:\n",
    "              yield '. '.join(chunk).strip() + '.'\n",
    "            chunk = [sentence]\n",
    "            word_count = words_in_sentence\n",
    "\n",
    "    # Yield the last chunk if it's not empty\n",
    "    if chunk:\n",
    "        yield ' '.join(chunk).strip() + '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1787df2-a63c-48ab-815f-919f81f4f969",
   "metadata": {},
   "source": [
    "## Get Embeddings for Each Chunk\n",
    "\n",
    "We need an embedding for each chunk to store it in the FAISS database. To get them, we pass the text chunks to the Jina AI embedding model endpoint, using the method `embedding_client.embed()`. Then, we add the text chunks and embedding vectors to the pandas dataframe `tu_delft_dataframe` as the new columns `chunks` and `embeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c15c8-d126-42a2-9ac3-a76016897854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_embeddings(text_df):\n",
    "    chunks = list(chunk_text(text_df['Text']))\n",
    "    embeddings = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "      response = embedding_client.embed(texts=[chunk])\n",
    "      chunk_embedding = response[0]['embedding']\n",
    "      embeddings.append(np.array(chunk_embedding))\n",
    "\n",
    "    text_df['chunks'] = chunks\n",
    "    text_df['embeddings'] = embeddings\n",
    "    return text_df\n",
    "\n",
    "print(\"Embedding text chunks ...\")\n",
    "\n",
    "tu_delft_dataframe = tu_delft_dataframe.progress_apply(generate_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec222d6-a5cd-4612-bdf8-5f3975bf4353",
   "metadata": {},
   "source": [
    "## Set Up Semantic Search Using Faiss\n",
    "\n",
    "The code below creates a FAISS database and inserts the chunks and embedding vectors by iterating over `tu_delft_pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30207d5b-2aa3-4eed-b203-2647f61ffa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = 768  # dimension of Jina v2 embeddings\n",
    "index_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
    "k = 0\n",
    "\n",
    "doc_ref = dict()\n",
    "\n",
    "for idx, row in tu_delft_dataframe.iterrows():\n",
    "    embeddings = row['embeddings']\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype='float32').reshape(1, -1))\n",
    "        faiss.normalize_L2(normalized_embedding)\n",
    "        index_with_ids.add_with_ids(normalized_embedding, k)\n",
    "        doc_ref[k] = (row['chunks'][i], idx)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7885a8a-4b43-41c8-ae58-89f2d5fa724b",
   "metadata": {},
   "source": [
    "# Start the Jina Reranker v1 Endpoint\n",
    "\n",
    "As with the Jina Embedding v2 model above, this code will launch an instance of ml.g4dn.xlarge on AWS to run the reranker model. Similarly, it may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb24b9-2c3a-4f7a-8de9-b1368d92d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from jina_sagemaker import Client\n",
    "\n",
    "# Choose a name for your reranker endpoint. It can be anything convenient.\n",
    "reranker_endpoint_name = \"jina_reranker\"\n",
    "\n",
    "reranker_client = Client(region_name=region)\n",
    "reranker_client.create_endpoint(\n",
    "    arn=reranker_package_arn,\n",
    "    role=role,\n",
    "    endpoint_name=reranker_endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    n_instances=1,\n",
    ")\n",
    "\n",
    "reranker_client.connect_to_endpoint(endpoint_name=reranker_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad67cc-db59-469c-af80-f79466a5ca4f",
   "metadata": {},
   "source": [
    "# Define Query Functions\n",
    "\n",
    "Next, we will define a function that identifies the most similar transcript chunks to any text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec66aa9-92d4-4e11-a1ce-9361e4fbdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_transcript_segment(query, n=20):\n",
    "    query_embedding = embedding_client.embed(texts=[query])[0]['embedding']  # Assuming the query is short enough to not need chunking\n",
    "    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype='float32').reshape(1, -1))\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    D, I = index_with_ids.search(query_embedding, n)  # Get the top n matches\n",
    "\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        distance = D[0][i]\n",
    "        index_id = I[0][i]\n",
    "        transcript_segment, doc_idx = doc_ref[index_id]\n",
    "        results.append((transcript_segment, doc_idx, distance))\n",
    "\n",
    "    # Sort the results by score, highest to lowest\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return [(tu_delft_dataframe.iloc[r[1]][\"Title\"].strip(), r[0], r[2]) for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d110b-dd1a-4e78-8ab7-f47b16fe7f71",
   "metadata": {},
   "source": [
    "Also, define a function that accesses the reranker endpoint `reranker_client` and is set up to accept the output of `find_most_similar_transcript_segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78086f47-65d5-40b9-94db-fbb0b6ec05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query_found, query, n=3):\n",
    "    ret = reranker_client.rerank(\n",
    "        documents=[f[1] for f in query_found], \n",
    "        query=query, \n",
    "        top_n=n,\n",
    "    )\n",
    "    return [query_found[r['index']] for r in ret[0]['results']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80182f-4b6d-4c93-b76e-ef9e01cdbaf5",
   "metadata": {},
   "source": [
    "# Mistral-Instruct with JumpStart\n",
    "\n",
    "For this tutorial, we will use the `mistral-7b-instruct` model, which is [available via SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/), as the LLM portion of the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73181738-5743-4d54-b665-e17a68dba344",
   "metadata": {},
   "source": [
    "## Loading Mistral-Instruct with JumpStart\n",
    "To load the model with JumpStart, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eae130-9fc0-4c0f-a7ac-fb2ddd5a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "jumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\n",
    "model_predictor = jumpstart_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881ba44-85a1-4bcb-8539-7c3df4c292e7",
   "metadata": {},
   "source": [
    "## Making a Prompt Template for Mistral-Instruct\n",
    "\n",
    "Below is the code to create a prompt template for Mistral-Instruct for this application using [Python’s built-in string template class](https://docs.python.org/3/library/string.html#template-strings). It assumes that for each query there are three matching transcript chunks that will be presented to the model.\n",
    "\n",
    "You can experiment with this template yourself to modify this application or see if you can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be4f6c-b1c1-4982-b7a6-b23df19f8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt_template = Template(\"\"\"\n",
    "  <s>[INST] Answer the question below only using the given context.\n",
    "  The question from the user is based on transcripts of videos from a YouTube\n",
    "    channel.\n",
    "  The context is presented as a ranked list of information in the form of\n",
    "    (video-title, transcript-segment), that is relevant for answering the\n",
    "    user's question.\n",
    "  The answer should only use the presented context. If the question cannot be\n",
    "    answered based on the context, say so.\n",
    "\n",
    "  Context:\n",
    "  1. Video-title: $title_1, transcript-segment: $segment_1\n",
    "  2. Video-title: $title_2, transcript-segment: $segment_2\n",
    "  3. Video-title: $title_3, transcript-segment: $segment_3\n",
    "\n",
    "  Question: $question\n",
    "\n",
    "  Answer: [/INST]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1a03c-481d-4a5a-b5db-289cc1c82381",
   "metadata": {},
   "source": [
    "# Querying the Model\n",
    "\n",
    "We now have all the parts of a complete RAG application and can start querying it. Querying the model is a three-step process.\n",
    "\n",
    "1. Search for relevant chunks given a query.\n",
    "2. Assemble the prompt.\n",
    "3. Send the prompt to the Mistral-Instruct model and return its answer.\n",
    "\n",
    "To search for relevant chunks, we use the `find_most_similar_transcript_segment` function we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983504-81a7-41ac-8edc-44f7c33d2990",
   "metadata": {},
   "source": [
    "Fist, we query for related segments from the video transcripts and rerank the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f40fa-b898-4b0f-866a-80b4b5150b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was the first offshore wind farm commissioned?\"\n",
    "search_results = find_most_similar_transcript_segment(question)\n",
    "reranked_results = rerank_results(search_results, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b819b-1a3b-4c1d-8bdb-10e78f0a7405",
   "metadata": {},
   "source": [
    "You can inspect the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c1c1c-f3b8-4181-a5c5-3c973f3dbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, text, _ in reranked_results:\n",
    "    print(title + \"\\n\" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e79c4c-bcb6-4283-b7be-65b4cf857660",
   "metadata": {},
   "source": [
    "Next, we instanciate the template and fill in the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819bd20-839a-4d63-b56c-deb56e31d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_llm = prompt_template.substitute(\n",
    "    question = question,\n",
    "    title_1 = search_results[0][0],\n",
    "    segment_1 = search_results[0][1],\n",
    "    title_2 = search_results[1][0],\n",
    "    segment_2 = search_results[1][1],\n",
    "    title_3 = search_results[2][0],\n",
    "    segment_3 = search_results[2][1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54432f-618f-4013-ae8d-9f818a75d9c5",
   "metadata": {},
   "source": [
    "Inspect the completed prompt text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003e611-e1c0-47eb-8797-065c8e5162c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_for_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb7b91-5b58-4d6c-aef5-25dc43692db5",
   "metadata": {},
   "source": [
    "Now, we can pass the complete prompt to the language model endpoint `model_predictor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2168f1d-45e9-47e2-9aee-b8dd8b651c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model_predictor.predict({\"inputs\": prompt_for_llm})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d49ac-6d36-488d-b6b2-42f2ce4a4fd6",
   "metadata": {},
   "source": [
    "Print the resulting answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c0739-402a-4608-9e16-a815a97243f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answer[0]['generated_text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7b7a2-19c1-4f0b-bf78-b52c7047d1e0",
   "metadata": {},
   "source": [
    "Let’s simplify querying by writing a function to do all the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607ae6-de7f-4672-a1a7-a41246fe5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question):\n",
    "    search_results = find_most_similar_transcript_segment(question)\n",
    "    reranked_results = rerank_results(search_results, question)\n",
    "    prompt_for_llm = prompt_template.substitute(\n",
    "        question = question,\n",
    "        title_1 = search_results[0][0],\n",
    "        segment_1 = search_results[0][1],\n",
    "        title_2 = search_results[1][0],\n",
    "        segment_2 = search_results[1][1],\n",
    "        title_3 = search_results[2][0],\n",
    "        segment_3 = search_results[2][1],\n",
    "    )\n",
    "    answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n",
    "    return answer[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b0353-eb3d-4862-a01e-8088cc8f4074",
   "metadata": {},
   "source": [
    "Now let's ask some questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464b9d-a614-4e3e-b73a-af5ffec6c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"Who is Reneville Solingen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227ce69-ef85-4455-8d7c-364b2431acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"What is a Kaplan Meyer estimator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88187c6-fa15-46df-8e5e-e2c3b92c9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"What countries export the most coffee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2181f-f128-42f5-b106-92a5185f7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346f6b5-e448-43be-a39d-606e217d5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"What is the European Green Deal?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c973901-ca46-4d60-96e3-1eb5f2df9ba5",
   "metadata": {},
   "source": [
    "# Shutting Down\n",
    "\n",
    "Because you are billed by the hour for the models you use and for the AWS infrastructure to run them, it is very important, when you finish, to stop all three AI models used in this tutorial:\n",
    "\n",
    "- The embedding model endpoint `embedding_client`\n",
    "- The reranker model endpoint `reranker_client`\n",
    "- The large language model endpoint `model_predictor`\n",
    "\n",
    "To shut all three model endpoints down, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f8545-0869-4033-b7f9-2408c0f7d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End all clients\n",
    "\n",
    "embedding_client.delete_endpoint()\n",
    "embedding_client.close()\n",
    "reranker_client.delete_endpoint()\n",
    "reranker_client.close()\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
