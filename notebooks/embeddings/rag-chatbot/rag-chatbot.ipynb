{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot that Doesn't Suck\n",
    "\n",
    "In this notebook we'll build a RAG-based chatbot for a small furniture manufacturer in Oahu, Hawaii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites\n",
    "\n",
    "### Install Pandoc\n",
    "\n",
    "This notebook requires [Pandoc](https://pandoc.org/) to be installed on your system, to convert the furniture company's HTML pages to markdown format. The cell below will check if it's installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Pandoc is already installed. You're good to go!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import sys\n",
    "from IPython.display import HTML\n",
    "\n",
    "def check_pandoc():\n",
    "    if not shutil.which(\"pandoc\"):\n",
    "        return HTML(\"Please <a href=\\\"https://pandoc.org/installing.html\\\">install Pandoc</a> before continuing\")\n",
    "    else:\n",
    "        return HTML(\"Pandoc is already installed. You're good to go!\")\n",
    "\n",
    "check_pandoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iShApPi-09Xj"
   },
   "source": [
    "### Set Up Authorization Tokens\n",
    "\n",
    "In this notebook we'll use:\n",
    "\n",
    "- [Jina Embeddings v2](https://jina.ai/embeddings/)\n",
    "- [Hugging Face Inference API](https://huggingface.co/settings/tokens) (token link)\n",
    "\n",
    "You'll need to get tokens for each of the above and enter them below.\n",
    "\n",
    "**Note:** For Hugging Face token, please choose finegrained permissions and enable _Make calls to the serverless Inference API_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2agYA5_TyX5",
    "outputId": "19443e35-12af-4f27-b14b-346e27a37761"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "jinaai_api_key = getpass(prompt=\"Your Jina Embeddings API key: \")\n",
    "hf_inference_api_key = getpass(prompt=\"Your Hugging Face Inference API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSvL4dpfUJur",
    "outputId": "188f1d5f-7618-4693-9a74-e44103b8ff6a"
   },
   "outputs": [],
   "source": [
    "# RAG dependencies\n",
    "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-jinaai llama-index-llms-huggingface \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUMsMe3TTTNu"
   },
   "source": [
    "## Process data\n",
    "\n",
    "We used GPT to generate some sample data for a fictitious small furniture maker in Oahu, Hawaii. This consists of four simple HTML pages:\n",
    "\n",
    "- [Front page](https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/front.html)\n",
    "- [Product listings page](https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/products.html)\n",
    "- [FAQ page](https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/faq.html)\n",
    "- [Contact page](https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/contact.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tuHsDJ3nLLwP"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import subprocess\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "SktkIQxcLpi4"
   },
   "outputs": [],
   "source": [
    "# cleanup from last run\n",
    "!rm -rf {data_dir}\n",
    "!mkdir {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_urls = [\n",
    "    \"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/front.html\",\n",
    "    \"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/products.html\",\n",
    "    \"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/faq.html\",\n",
    "    \"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/rag-chatbot/data/contact.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files = []\n",
    "\n",
    "for url in html_urls:\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    html = requests.get(url).content\n",
    "    html_files.append(file_path) # store path in a list for future processing\n",
    "\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Markdown\n",
    "\n",
    "HTML is a pain to break into chunks and unreliable for LLMs to parse. We'll convert it to [markdown]() to make things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X-IgrP6Nz1f"
   },
   "outputs": [],
   "source": [
    "# convert html files to markdown for easier chunking\n",
    "for filename in html_files:\n",
    "  base_name = os.path.splitext(filename)[0]\n",
    "  md_file = os.path.join(base_name + \".md\")\n",
    "\n",
    "  # Colab uses ancient pandoc, with different argument for markdown header style\n",
    "  try:\n",
    "    # colab pandoc\n",
    "    subprocess.run([\"pandoc\", \"--atx-headers\", filename, \"-o\", md_file], check=True)\n",
    "  except:\n",
    "    # newer pandoc\n",
    "    subprocess.run([\"pandoc\", \"--markdown-headings=atx\", filename, \"-o\", md_file], check=True)\n",
    "\n",
    "md_files = glob(f'{data_dir}/*.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break Pages into Chunks\n",
    "\n",
    "We'll make the data more digestible to our chatbot by breaking it into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkpAyY4rQkaI"
   },
   "outputs": [],
   "source": [
    "# break markdown files into chunks\n",
    "docs = []\n",
    "\n",
    "for md_file in md_files:\n",
    "    with open(md_file, 'r') as f:\n",
    "          \n",
    "        content = f.read()\n",
    "        docs.append(content) # add full page\n",
    "    \n",
    "        content_chunks = content.split(\"\\n#\")\n",
    "        docs.extend(content_chunks) # add individual sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_GVRPqVTw5T"
   },
   "source": [
    "## Build RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZZ1ZtLtYel3"
   },
   "source": [
    "### Access Jina Embeddings v2 via the LlamaIndex interface.\n",
    "\n",
    "This code creates the LlamaIndex object that manages your connection to the Jina Embeddings v2 API.\n",
    "\n",
    "The resulting object is held in the variable `jina_embedding_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itaj6kwwUvki"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    api_key=jinaai_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJynvz-VYaPw"
   },
   "source": [
    "### Access the Mixtral Model via the HuggingFace Inference API\n",
    "\n",
    "This code creates a holder for accessing the `mistralai/Mixtral-8x7B-Instruct-v0.1` model via the Hugging Face Inference API. The resulting object is held in the variable `mixtral_llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwElcMppVWqj",
    "outputId": "61999353-d80a-4d8f-c179-6e995f888a85"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=hf_inference_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUJfrfJZasMS"
   },
   "source": [
    "### Convert Chunks To Be Suitable for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH3fEHdauze"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.readers import StringIterableReader\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "chunks = StringIterableReader().load_data(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoCxdn7BYkD_"
   },
   "source": [
    "### Create a Service\n",
    "\n",
    "The code creates a RAG service that has access to Jina Embeddings and Mixtral Instruct and stores it in the variable `service_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5zL2rmyYmes",
    "outputId": "436bbe5e-af7b-4128-fd2b-e8e61637c25c"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJYsOxGDYqHV"
   },
   "source": [
    "### Build the Document Index\n",
    "\n",
    "Next, we store the documents in LlamaIndex' `VectorStoreIndex`, generating embeddings with Jina Embeddings v2 model and using them as keys for retrieval.\n",
    "\n",
    "**Note:** this may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQcAN4lIVaD8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=chunks, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Att4O7CuYxOl"
   },
   "source": [
    "### Prepare a Prompt Template\n",
    "\n",
    "This is the prompt template that will be presented to Mixtral Instruct, with `{context_str}` and `{query_str}` replaced with the retrieved documents and your query respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYXfmEriYxiB"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query. Please be brief, concise, and complete.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"I'm sorry, but we don't have any information about that. Please contact us on info@oahufurniture.com for more information.\\\".\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ip0RS-LZDfY"
   },
   "source": [
    "### Assemble the Full Query Engine\n",
    "\n",
    "The query engine has three parts:\n",
    "\n",
    "* `retriever` is the search engine that takes user requests and retrieves relevant documents from the vector store.\n",
    "* `response_synthesizer` uses the prompt created above to join the retrieved documents and user request and passes them to the LLM, getting back its response.\n",
    "* `query_engine` is a container object that holds the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-5SMeo8YznS"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbBmF1vObopZ"
   },
   "source": [
    "## Ask Some Questions\n",
    "\n",
    "Let's ask some queries to see our chatbot in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def get_answer(question):\n",
    "    result = query_engine.query(question)\n",
    "    return HTML(result.response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Relevant Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3E9lsW5bq3b",
    "outputId": "32e86ceb-16fd-448d-be04-bfae4e256679"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What kind of furniture do you make?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-W7AYjcf0V",
    "outputId": "1b18cda7-95a6-432f-8028-58e9d096d915"
   },
   "outputs": [],
   "source": [
    "get_answer(\"How much does your furniture cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXYodN8aco-5",
    "outputId": "bb9fdcd2-b15d-4d16-c60d-4d55af79f9b9"
   },
   "outputs": [],
   "source": [
    "get_answer(\"Can I see your furniture in person?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZ11omuWdAhF",
    "outputId": "5438b069-5da0-4a21-ef42-add80386b607"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What payment methods do you accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHdY2_MrdMV1"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What is your furniture made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Irrelevant Question\n",
    "\n",
    "We want to be sure our chatbot _won't_ answer irrelevant questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"How is a computer useful on a farm?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions in Different Languages\n",
    "\n",
    "Although the language model is specifically for English, it's often possible to get answers in other languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyN7ue3nddSr"
   },
   "outputs": [],
   "source": [
    "# German\n",
    "get_answer(\"Was für Möbel stellen Sie her?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Quw9FaH7dUSS"
   },
   "outputs": [],
   "source": [
    "# Simplified Chinese\n",
    "get_answer(\"你的家具是用什么材料制成的？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Your Own Questions\n",
    "\n",
    "Enter your question below and then hit enter to send. The question should be generated within a few seconds. Type \"stop\" to quit the question loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Please enter your question: \")\n",
    "    if question.lower() == \"stop\":\n",
    "        break\n",
    "    answer = get_answer(question).data\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
