{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot that Doesn't Suck\n",
    "\n",
    "In this notebook we'll build a RAG-based chatbot for a small furniture manufacturer in Oahu, Hawaii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iShApPi-09Xj"
   },
   "source": [
    "## Set Up Authorization Tokens\n",
    "\n",
    "In this notebook we'll use:\n",
    "\n",
    "- [Jina Embeddings v2](https://jina.ai/embeddings/)\n",
    "- [Hugging Face Inference API](https://huggingface.co/settings/tokens) (token link)\n",
    "\n",
    "You'll need to get tokens for each of the above and enter them below.\n",
    "\n",
    "**Note:** For Hugging Face token, please choose finegrained permissions and enable _Make calls to the serverless Inference API_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2agYA5_TyX5",
    "outputId": "19443e35-12af-4f27-b14b-346e27a37761"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "jinaai_api_key = getpass(prompt=\"Your Jina Embeddings API key: \")\n",
    "hf_inference_api_key = getpass(prompt=\"Your Hugging Face Inference API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSvL4dpfUJur",
    "outputId": "188f1d5f-7618-4693-9a74-e44103b8ff6a"
   },
   "outputs": [],
   "source": [
    "# RAG dependencies\n",
    "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-jinaai llama-index-llms-huggingface \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUMsMe3TTTNu"
   },
   "source": [
    "## Process data\n",
    "\n",
    "We used GPT to generate some sample data for a fictitious small furniture maker in Oahu, Hawaii. This consists of four simple HTML pages:\n",
    "\n",
    "- FAQ page\n",
    "- Front page\n",
    "- Contact page\n",
    "- Product listings page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuHsDJ3nLLwP"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SktkIQxcLpi4"
   },
   "outputs": [],
   "source": [
    "# cleanup from last run\n",
    "!rm -rf data\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MjeH2ViMHpQ"
   },
   "outputs": [],
   "source": [
    "# download html files\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/faq.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/front.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/contact.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/products.html --directory-prefix data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFlCKDQNLSEi"
   },
   "outputs": [],
   "source": [
    "# store html files in list\n",
    "data_dir = \"./data\"\n",
    "html_files = glob(f'{data_dir}/*.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Markdown\n",
    "\n",
    "HTML is a pain to break into chunks and unreliable for LLMs to parse. We'll convert it to [markdown]() to make things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X-IgrP6Nz1f"
   },
   "outputs": [],
   "source": [
    "# convert html files to markdown for easier chunking\n",
    "for filename in html_files:\n",
    "  base_name = os.path.splitext(filename)[0]\n",
    "  md_file = os.path.join(base_name + \".md\")\n",
    "\n",
    "  # Colab uses ancient pandoc, with different argument for markdown header style\n",
    "  try:\n",
    "    # colab pandoc\n",
    "    subprocess.run([\"pandoc\", \"--atx-headers\", filename, \"-o\", md_file], check=True)\n",
    "  except:\n",
    "    # newer pandoc\n",
    "    subprocess.run([\"pandoc\", \"--markdown-headings=atx\", filename, \"-o\", md_file], check=True)\n",
    "\n",
    "md_files = glob(f'{data_dir}/*.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break Pages into Chunks\n",
    "\n",
    "We'll make the data more digestible to our chatbot by breaking it into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkpAyY4rQkaI"
   },
   "outputs": [],
   "source": [
    "# break markdown files into chunks\n",
    "docs = []\n",
    "\n",
    "for md_file in md_files:\n",
    "    with open(md_file, 'r') as f:\n",
    "          \n",
    "        content = f.read()\n",
    "        docs.append(content) # add full page\n",
    "    \n",
    "        content_chunks = content.split(\"\\n#\")\n",
    "        docs.extend(content_chunks) # add individual sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_GVRPqVTw5T"
   },
   "source": [
    "## Build RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZZ1ZtLtYel3"
   },
   "source": [
    "### Access Jina Embeddings v2 via the LlamaIndex interface.\n",
    "\n",
    "This code creates the LlamaIndex object that manages your connection to the Jina Embeddings v2 API.\n",
    "\n",
    "The resulting object is held in the variable `jina_embedding_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itaj6kwwUvki"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    api_key=jinaai_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJynvz-VYaPw"
   },
   "source": [
    "### Access the Mixtral Model via the HuggingFace Inference API\n",
    "\n",
    "This code creates a holder for accessing the `mistralai/Mixtral-8x7B-Instruct-v0.1` model via the Hugging Face Inference API. The resulting object is held in the variable `mixtral_llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwElcMppVWqj",
    "outputId": "61999353-d80a-4d8f-c179-6e995f888a85"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=hf_inference_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUJfrfJZasMS"
   },
   "source": [
    "### Convert Chunks To Be Suitable for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH3fEHdauze"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.readers import StringIterableReader\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "chunks = StringIterableReader().load_data(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoCxdn7BYkD_"
   },
   "source": [
    "### Create a Service\n",
    "\n",
    "The code creates a RAG service that has access to Jina Embeddings and Mixtral Instruct and stores it in the variable `service_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5zL2rmyYmes",
    "outputId": "436bbe5e-af7b-4128-fd2b-e8e61637c25c"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJYsOxGDYqHV"
   },
   "source": [
    "### Build the Document Index\n",
    "\n",
    "Next, we store the documents in LlamaIndex' `VectorStoreIndex`, generating embeddings with Jina Embeddings v2 model and using them as keys for retrieval.\n",
    "\n",
    "**Note:** this may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQcAN4lIVaD8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=chunks, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Att4O7CuYxOl"
   },
   "source": [
    "### Prepare a Prompt Template\n",
    "\n",
    "This is the prompt template that will be presented to Mixtral Instruct, with `{context_str}` and `{query_str}` replaced with the retrieved documents and your query respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYXfmEriYxiB"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query. Please be brief, concise, and complete.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"I'm sorry, but we don't have any information about that. Please contact us on info@oahufurniture.com for more information.\\\".\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ip0RS-LZDfY"
   },
   "source": [
    "### Assemble the Full Query Engine\n",
    "\n",
    "The query engine has three parts:\n",
    "\n",
    "* `retriever` is the search engine that takes user requests and retrieves relevant documents from the vector store.\n",
    "* `response_synthesizer` uses the prompt created above to join the retrieved documents and user request and passes them to the LLM, getting back its response.\n",
    "* `query_engine` is a container object that holds the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-5SMeo8YznS"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbBmF1vObopZ"
   },
   "source": [
    "## Ask Some Questions\n",
    "\n",
    "Let's ask some queries to see our chatbot in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def get_answer(question):\n",
    "    result = query_engine.query(question)\n",
    "    return HTML(result.response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Relevant Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3E9lsW5bq3b",
    "outputId": "32e86ceb-16fd-448d-be04-bfae4e256679"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What kind of furniture do you make?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-W7AYjcf0V",
    "outputId": "1b18cda7-95a6-432f-8028-58e9d096d915"
   },
   "outputs": [],
   "source": [
    "get_answer(\"How much does your furniture cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXYodN8aco-5",
    "outputId": "bb9fdcd2-b15d-4d16-c60d-4d55af79f9b9"
   },
   "outputs": [],
   "source": [
    "get_answer(\"Can I see your furniture in person?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZ11omuWdAhF",
    "outputId": "5438b069-5da0-4a21-ef42-add80386b607"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What payment methods do you accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHdY2_MrdMV1"
   },
   "outputs": [],
   "source": [
    "get_answer(\"What is your furniture made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Irrelevant Question\n",
    "\n",
    "We want to be sure our chatbot _won't_ answer irrelevant questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"How is a computer useful on a farm?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions in Different Languages\n",
    "\n",
    "Although the language model is specifically for English, it's often possible to get answers in other languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Quw9FaH7dUSS"
   },
   "outputs": [],
   "source": [
    "# Simplified Chinese\n",
    "get_answer(\"你的家具是用什么材料制成的？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyN7ue3nddSr"
   },
   "outputs": [],
   "source": [
    "# German\n",
    "get_answer(\"Welche Zahlungsmethoden werden akzeptiert?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Your Own Questions\n",
    "\n",
    "Enter your question below and then hit enter to send. The question should be generated within a few seconds. Type \"stop\" to quit the question loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Please enter your question: \")\n",
    "    if question.lower() == \"stop\":\n",
    "        break\n",
    "    answer = get_answer(question).data\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVnt2ya0mwk6"
   },
   "source": [
    "## Set up API for external access\n",
    "\n",
    "If you're running this locally in a Jupyter notebook (i.e. **_not_ Google Colab**) you can test the chatbot via a RESTful API and simple web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throw exception if running in Colab. That's better behavior than trying to execute everything which won't work properly.\n",
    "import sys\n",
    "\n",
    "def in_colab():\n",
    "    return 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab():\n",
    "    raise RuntimeError(\"You are in Colab, stopping execution, since Colab doesn't support the next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4dK1B56lOG1"
   },
   "outputs": [],
   "source": [
    "!pip install -q fastapi uvicorn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "origins = [\"*\"] # all origins\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,  # Allows all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allows all methods\n",
    "    allow_headers=[\"*\"],  # Allows all headers\n",
    ")\n",
    "\n",
    "# Define FastAPI routes\n",
    "\n",
    "@app.post('/')\n",
    "async def chat_endpoint(request: Request):\n",
    "    data = await request.json()  # Get JSON data from the request\n",
    "    response_data = {\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": get_answer(data[\"question\"])\n",
    "    }\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "@app.post('/shutdown')\n",
    "async def shutdown():\n",
    "    global server_running\n",
    "    server_running = False\n",
    "    def stop_uvicorn():\n",
    "        uvicorn_server.should_exit = True\n",
    "    threading.Thread(target=stop_uvicorn).start()\n",
    "    return {\"message\": \"Server shutting down...\"}\n",
    "\n",
    "# Start the FastAPI server in a new thread\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "threading.Thread(target=run).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in browser\n",
    "\n",
    "We can open a simple HTML chatbot page for you to test out the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexcg1/rag-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./rag-chatbot/web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import os\n",
    "\n",
    "web_port = 8000\n",
    "\n",
    "handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "with socketserver.TCPServer((\"\", web_port), handler) as httpd:\n",
    "    print(\"Serving at port\", web_port)\n",
    "    httpd.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can open your web browser to [http://localhost:8000](http://localhost:8000) to play with the chatbot in your browser.\n",
    "\n",
    "To stop the web server, go to _Kernel_ -> _Interrupt Kernel_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def stop_server():\n",
    "    response = requests.post(f\"http://localhost:{port}/shutdown\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_server()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
